# -*- coding: utf-8 -*-
"""val.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qEsTev0uEKFI6beI1TX0BKMpZGOqIAdH
"""

# !pip install -U gdown
# import gdown

# file_id = "1YFs_NLzPMqAzZekZYHxykG_qp2vg6qdP"
# gdown.download(f"https://drive.google.com/uc?id={file_id}", output="data.zip", quiet=False)

# import zipfile
# with zipfile.ZipFile("data.zip", 'r') as zip_ref:
#     zip_ref.extractall("data")



import argparse
import numpy as np
import pandas as pd
import cv2
import matplotlib.pyplot as plt
import os
import json
import xml.etree.ElementTree as ET
import glob

import torch
import torch.nn.functional as F
from torch import nn
from sklearn.metrics import roc_auc_score
from tqdm import tqdm
from torch.utils.data import DataLoader
from transformers import get_cosine_schedule_with_warmup
from torch.utils.data import Dataset

import cv2, glob, json, math, os, random, xml.etree.ElementTree as ET
from pathlib import Path
import numpy as np

from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

#data_root="/content/data"

img_size=256
device   = "cuda" if torch.cuda.is_available() else "cpu"



"""# Model"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import timm

class UniHeadNet(nn.Module):
    def __init__(self,
                 n_det_cls=10, n_seg_cls=21, n_cls=30,
                 n_anchor=3):
        super().__init__()
        self.backbone = timm.create_model(
            'efficientnet_b0',
            pretrained=True,
            features_only=True,
            out_indices=(3,)  # block6 output: (B, 320, 16, 16) for 256x256 input
        )
        #3 112 2 40
        self.reduce = nn.Sequential(
            nn.Conv2d(112, 256, kernel_size=1, bias=False),
            nn.BatchNorm2d(256),
            nn.SiLU()
        )
#         self.upsample = nn.Upsample(scale_factor=2, mode='bilinear',
#                                     align_corners=False)

        self.head_conv = nn.Sequential(
            nn.Conv2d(256, 256, 3, padding=1, bias=False),
            nn.BatchNorm2d(256), nn.SiLU())

        C_det = n_anchor * (5 + n_det_cls)
        C_seg = n_seg_cls
        C_cls = n_cls
        self.C_det, self.C_seg, self.C_cls = C_det, C_seg, C_cls

        self.head_out = nn.Conv2d(256, C_det + C_seg + C_cls, 1)

    def forward(self, x, export_onnx=False):
#         m = self.backbone
#         feat = m.forward_features(x)             # (B,1280,7,7)
        feat = self.backbone(x)[0]
        #print(feat.shape)
        #print(feat.shape)
        #feat = self.upsample(self.reduce(feat))
        feat = self.reduce(feat) # (B,256,14,14)
        #print(feat.shape)
        y = self.head_out(self.head_conv(feat))  # (B, C_all, 14, 14)

        #-------------- 拆解三種輸出 ----------------#
        B = y.size(0)
        C_det, C_seg, C_cls = self.C_det, self.C_seg, self.C_cls

        det_logits = y[:, :C_det]                    # (B,C_det,14,14)

        seg_logits = y[:, C_det:C_det + C_seg]       # (B,C_seg,14,14)
        seg_logits = F.interpolate(seg_logits, size=x.shape[2:],
                                   mode='bilinear', align_corners=False)

        cls_logits = y[:, -C_cls:]                  # (B,C_cls,14,14)
        cls_logits = cls_logits.mean(dim=[2, 3])     # GAP → (B,C_cls)

        if export_onnx:      # 方便部署
            return det_logits, seg_logits, cls_logits
        return {
            'det': det_logits,      # 後處理：decode boxes + NMS
            'seg': seg_logits,      # CrossEntropy2d
            'cls': cls_logits       # CE / focal loss
        }



"""# Seg"""

def auto_s_per_it(iterable, desc=None, **kwargs):
    bar = tqdm(iterable, desc=desc,
               bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {postfix}]",
               **kwargs)
    for item in bar:
        rate = bar.format_dict.get("rate")
        if rate and rate != float("inf"):
            sec = 1.0 / rate
            bar.set_postfix_str(f"{sec*1000:.0f} ms/it" if sec < 1 else f"{sec:.2f} s/it")
        yield item

def build_class_dict(xml_dir):
    cls_set = set()
    for p in glob.glob(f'{xml_dir}/*.xml'):
        root = ET.parse(p).getroot()
        name = root.find('.//object/name').text.strip()
        cls_set.add(name)
    cls2id = {c:i+1 for i,c in enumerate(sorted(cls_set))}
    return cls2id

cls2id = {'aeroplane': 1,
 'bicycle': 2,
 'bird': 3,
 'boat': 4,
 'bottle': 5,
 'bus': 6,
 'car': 7,
 'cat': 8,
 'chair': 9,
 'cow': 10,
 'dog': 11,
 'horse': 12,
 'motorbike': 13,
 'person': 14,
 'sheep': 15,
 'sofa': 16,
 'train': 17,
 'tvmonitor': 18}

#cls2id

import xml.etree.ElementTree as ET
import cv2, torch
from torch.utils.data import Dataset
import numpy as np

class SegMiniN(Dataset):
    def __init__(self, names, root, cls2id, size=256, lb=True):
        """
        names : list[str]   # 不含副檔名
        root  : Path        # 資料根目錄，下有 JPEGImages / SegmentationClass / Annotations
        """
        self.names, self.root = names, Path(root)
        self.size, self.lb = size, lb
        self.cls2id = cls2id    # 已事先建立的 {class: id}

    def _letterbox(self, img, mask):
        h, w = img.shape[:2]
        s = self.size / max(h, w)
        nh, nw = int(h * s), int(w * s)
        img_r = cv2.resize(img, (nw, nh))
        msk_r = cv2.resize(mask, (nw, nh), interpolation=cv2.INTER_NEAREST)

        out_i = np.zeros((self.size, self.size, 3), np.uint8)
        out_m = np.zeros((self.size, self.size),   np.uint8)
        top, left = (self.size - nh)//2, (self.size - nw)//2
        out_i[top:top+nh, left:left+nw] = img_r
        out_m[top:top+nh, left:left+nw] = msk_r
        return out_i, out_m

    def __getitem__(self, idx):
        name = self.names[idx]

        img = cv2.imread(f"{self.root}/JPEGImages/{name}.jpg")[:, :, ::-1]  # BGR→RGB

        # 讀 mask；如是彩色 palette，這裡簡化：只判斷 >0 為前景
        mask_png = cv2.imread(f"{self.root}/SegmentationClass/{name}.png",
                              cv2.IMREAD_UNCHANGED)


        target_color = np.array([192, 224, 224], dtype=np.uint8)

        # 建立布林遮罩：True 表示「這個像素就是目標顏色」
        mask = (mask_png  == target_color).all(axis=-1)   # shape = (512, 512)

        # 把符合條件的像素設成 [0,0,0]
        mask_png[mask] = [0, 0, 0]


        if mask_png.ndim == 3:

            mask_png = mask_png.sum(axis=2)

        # 讀 XML → 取 <name>
        xml_path = f"{self.root}/Annotations/{name}.xml"
        cls_name = ET.parse(xml_path).find('.//object/name').text.strip()
        fg_id = self.cls2id[cls_name]
        #print(fg_id)
        mask = np.where(mask_png > 0, fg_id, 0).astype(np.uint8)

        # 尺寸處理
        if self.lb:
            img, mask = self._letterbox(img, mask)
        else:
            img = cv2.resize(img, (self.size, self.size))
            mask = cv2.resize(mask, (self.size, self.size), interpolation=cv2.INTER_NEAREST)

        # to Tensor
        img = torch.from_numpy(img).permute(2,0,1).float() / 255.
        mask = torch.from_numpy(mask).long()        # 先轉成 int64
        mask_1hot = torch.nn.functional.one_hot(
            mask, num_classes=len(self.cls2id)+1      # (H,W,k)
        ).permute(2, 0, 1).float()

        return img, mask_1hot         # img:(3,256,256)  mask:(k,256,256)

    def __len__(self): return len(self.names)



@torch.no_grad()
def validate_seg(model, val_loader, device):
    model.eval()
    total_loss = 0
    total_mIoU = 0
    num_classes = val_loader.dataset[0][1].shape[0]  # (C, H, W)

    for img, mask_1hot in auto_s_per_it(val_loader):
        img = img.to(device)
        mask_1hot = mask_1hot.to(device)

        out = model(img)['seg']

        #loss = seg_loss_fn(out, mask_1hot)
        #total_loss += loss.item()

        # mIoU
        pred = torch.sigmoid(out) > 0.5
        iou = compute_mIoU(pred, mask_1hot)
        total_mIoU += iou.item()

    #avg_loss = total_loss / len(val_loader)
    avg_mIoU = total_mIoU / len(val_loader)
    #print(f"Val Loss: {avg_loss:.4f}, mIoU: {avg_mIoU:.4f}")
    return avg_mIoU
def compute_mIoU(pred, target, epsilon=1e-6):
    """
    pred: (B, C, H, W) binary after sigmoid > 0.5
    target: (B, C, H, W)
    """
    intersection = (pred & target.bool()).sum(dim=(2, 3))
    union = (pred | target.bool()).sum(dim=(2, 3))
    iou = (intersection + epsilon) / (union + epsilon)
    return iou.mean()



"""# Det"""

import torch
from torch.utils.data import Dataset
from PIL import Image
import os, json
import random
import torchvision.transforms as T
import torchvision.transforms.functional as TF

class COCODetectionDataset(Dataset):
    def __init__(self, img_dir, ann_path, det_cat_mapping, size=256):
        self.img_dir = img_dir
        self.ann_path = ann_path
        self.size = size
        self.det_cat_mapping = det_cat_mapping

        with open(ann_path, 'r') as f:
            coco = json.load(f)

        self.id2file = {img['id']: img['file_name'] for img in coco['images']}

        self.imgid2anns = {}
        for ann in coco['annotations']:
            if ann['iscrowd']:
                continue
            img_id = ann['image_id']
            if img_id not in self.imgid2anns:
                self.imgid2anns[img_id] = []
            self.imgid2anns[img_id].append(ann)

        self.image_ids = [k for k, v in self.imgid2anns.items() if len(v) > 0]

        # 圖像增強設定
        self.color_jitter = T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3)

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx):
        img_id = self.image_ids[idx]
        file_name = self.id2file[img_id]
        img_path = os.path.join(self.img_dir, file_name)

        img = Image.open(img_path).convert("RGB")
        orig_w, orig_h = img.size

        # 只取第一個 bbox
        ann = self.imgid2anns[img_id][0]
        x, y, w, h = ann['bbox']
        x1, y1, x2, y2 = x, y, x + w, y + h

        # 隨機水平翻轉
        if random.random() < 0.5:
            img = TF.hflip(img)
            x1, x2 = orig_w - x2, orig_w - x1  # 對 x1,x2 做鏡像

        # 隨機色彩抖動
        if random.random() < 0.5:
            img = self.color_jitter(img)

        # Resize 並同步 bbox 縮放
        img = TF.resize(img, (self.size, self.size))
        scale_x = self.size / orig_w
        scale_y = self.size / orig_h
        x1 *= scale_x
        x2 *= scale_x
        y1 *= scale_y
        y2 *= scale_y

        box = torch.tensor([[x1, y1, x2, y2]], dtype=torch.float32)

        original_id = ann['category_id']
        label = self.det_cat_mapping.get(original_id, -1)
        if label == -1:
            raise ValueError(f"Category ID {original_id} not in det_cat_mapping")
        label = torch.tensor([label], dtype=torch.int64)

        img_tensor = TF.to_tensor(img)

        target = {
            "boxes": box,             # (1, 4)
            "labels": label,          # (1,)
            "image_id": torch.tensor([img_id])
        }

        return img_tensor, target

# import json

# # 開啟並讀取 json 檔
# with open(f'{data_root}/data/mini_coco_det/annotations/instances_train2017.json', 'r', encoding='utf-8') as f:
#     data = json.load(f)

# # 現在 data 是一個 Python 字典 (dict) 或列表 (list)


# cat_id=[]
# for i in range(len(data["annotations"])):
#     cat_id.append(data["annotations"][i]["category_id"])

# det_cat_mapping = {item: idx for idx, item in enumerate(set(cat_id))}
det_cat_mapping = {5: 0, 70: 1, 7: 2, 11: 3, 13: 4, 16: 5, 85: 6, 23: 7, 24: 8, 25: 9}

rev_det_cat_mapping = {v: k for k, v in det_cat_mapping.items()}

def collate_fn(batch):
    imgs, targets = list(zip(*batch))
    imgs = torch.stack(imgs, dim=0)
    return imgs, list(targets)


def decode_boxes(det_logits, anchors, stride=16):
    """
    det_logits: (B, A*(5+C), H, W)
    anchors: (A, 2) anchor 尺寸
    return: decoded boxes in (B, A*H*W, 4), confs, class_probs
    """
    B, _, H, W = det_logits.shape
    A = anchors.shape[0]
    C = det_logits.shape[1] // A - 5  # 推出 class 數

    pred = det_logits.view(B, A, 5 + C, H, W).permute(0, 1, 3, 4, 2)  # (B,A,H,W,5+C)
    tx, ty, tw, th, obj = pred[..., 0], pred[..., 1], pred[..., 2], pred[..., 3], pred[..., 4]
    cls_logits = pred[..., 5:]

    grid_y, grid_x = torch.meshgrid(torch.arange(H), torch.arange(W), indexing='ij')
    grid = torch.stack((grid_x, grid_y), dim=-1).to(det_logits.device)  # (H,W,2)

    # sigmoid + decode
    cx = (grid[..., 0][None, None] + tx.sigmoid()) * stride
    cy = (grid[..., 1][None, None] + ty.sigmoid()) * stride
    w = (tw.exp() * anchors[:, 0][:, None, None]).to(det_logits.device)
    h = (th.exp() * anchors[:, 1][:, None, None]).to(det_logits.device)

    x1 = cx - w / 2
    y1 = cy - h / 2
    x2 = cx + w / 2
    y2 = cy + h / 2

    boxes = torch.stack([x1, y1, x2, y2], dim=-1)  # (B,A,H,W,4)
    boxes = boxes.reshape(B, -1, 4)
    confs = obj.sigmoid().reshape(B, -1)
    class_probs = cls_logits.softmax(-1).reshape(B, -1, C)

    return boxes, confs, class_probs




def compute_iou(box1, box2):
    """
    box1, box2: (4,) → [x1, y1, x2, y2]
    """
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])

    inter_area = max(x2 - x1, 0) * max(y2 - y1, 0)
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union_area = area1 + area2 - inter_area

    return inter_area / union_area if union_area > 0 else 0
import torch
from torchvision.ops import nms


@torch.no_grad()
def evaluate_map(model, val_loader, anchors, num_classes, device, iou_thresh=0.5, conf_thresh=0.5):
    model.eval()
    total = 0
    correct = 0

    for imgs, targets in auto_s_per_it(val_loader):
        imgs = imgs.to(device)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        outputs = model(imgs)
        det_logits = outputs['det']  # (B, A*(5+C), H, W)

        boxes, confs, class_probs = decode_boxes(det_logits, anchors.to(device))
        pred_scores = confs * class_probs.max(dim=-1).values  # (B, N)
        pred_labels = class_probs.argmax(dim=-1)              # (B, N)

        for i in range(len(imgs)):
            score = pred_scores[i]   # (N,)
            label = pred_labels[i]   # (N,)
            box = boxes[i]           # (N,4)

            keep = score > conf_thresh
            score = score[keep]
            label = label[keep]
            box = box[keep]

            # NMS
            if box.shape[0] > 0:
                keep_idx = nms(box, score, iou_threshold=0.5)
                box = box[keep_idx]
                label = label[keep_idx]

            # 只保留 top-1 預測
            if box.shape[0] > 0:
                pred_box = box[0].cpu()
                pred_label = label[0].cpu().item()
            else:
                pred_box = None

            gt_box = targets[i]['boxes'][0].cpu()
            gt_label = targets[i]['labels'][0].cpu().item()
            total += 1

            if pred_box is not None:
                iou = compute_iou(pred_box, gt_box)
                if iou > iou_thresh and pred_label == gt_label:
                    correct += 1

    precision = correct / total
    #print(f"mAP@0.5 (top-1 prediction only): {precision:.4f}")
    return precision

import torch.optim as optim
# 初始化

n_det_cls = len(det_cat_mapping)  # 假設 det_cat_mapping: 原始ID → 0開始
n_anchor = 3
anchors = torch.tensor([[1.0, 1.0], [2.0, 2.0], [0.5, 0.5]])  # (A, 2)

#model = UniHeadNet(n_det_cls=n_det_cls, n_anchor=n_anchor).to(device)



"""# cls"""

from torch.utils.data import Dataset
from torchvision import transforms
from pathlib import Path
from PIL import Image


from torchvision import transforms


cls_transform = transforms.Compose([
    transforms.Resize((img_size, img_size)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

class ImageClsDataset(Dataset):
    def __init__(self, root, transform):
        self.root = Path(root)
        self.paths = list(self.root.glob("*/*.jpeg"))
        self.transform = transform

    def __getitem__(self, idx):
        img_path = self.paths[idx]
        label = int(img_path.parent.name)
        img = Image.open(img_path).convert("RGB")
        img = self.transform(img)
        return img, label

    def __len__(self):
        return len(self.paths)

@torch.no_grad()
def evaluate_cls(model, dataloader, device):
    model.eval()
    total_loss, total_correct, total_samples = 0, 0, 0
    all_preds, all_labels = [], []

    for imgs, labels in auto_s_per_it(dataloader, desc="Val"):

        imgs, labels = imgs.to(device), labels.to(device)

        logits = model(imgs)['cls']
        y = F.one_hot(labels, num_classes=logits.shape[1]).float()

        loss = F.binary_cross_entropy_with_logits(logits, y)
        total_loss += loss.item() * imgs.size(0)

        probs = torch.sigmoid(logits)
        preds = torch.argmax(probs, dim=1)
        total_correct += (preds == labels).sum().item()
        total_samples += imgs.size(0)

        all_preds.append(probs.cpu())
        all_labels.append(y.cpu())

    all_preds = torch.cat(all_preds).numpy()
    all_labels = torch.cat(all_labels).numpy()

#     try:
#         auc = roc_auc_score(all_labels, all_preds, average='macro', multi_class='ovr')
#     except:
#         auc = 0.0

    acc = total_correct / total_samples
    #avg_loss = total_loss / total_samples
    #return avg_loss, acc, auc
    return  acc

def main(args):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = UniHeadNet(n_seg_cls=19,n_det_cls=10, n_anchor=3,n_cls=30).to(device)
    model.load_state_dict(torch.load(args.weights))
    results = {}

    if args.tasks in ("all", "det"):


        val_dataset = COCODetectionDataset(
            img_dir=f'{args.data_root}/data/mini_coco_det/val',
            ann_path=f'{args.data_root}/data/mini_coco_det/annotations/instances_val2017.json',
            det_cat_mapping=det_cat_mapping,
            size=img_size
        )

        val_det_loader = DataLoader(
            val_dataset, batch_size=1, shuffle=False,
            collate_fn=collate_fn, num_workers=2
        )
        results["mAP"] = evaluate_map(model, val_det_loader, anchors, num_classes=n_det_cls, device=device)

    if args.tasks in ("all", "seg"):


        val_names= sorted(p.stem for p in Path(f"{args.data_root}/data/mini_voc_seg/val/Annotations").glob("*.xml"))
        val_ds   = SegMiniN(val_names,  f"{args.data_root}/data/mini_voc_seg/val", cls2id, img_size)
        val_seg_loader = DataLoader(val_ds, batch_size=1)


        results["mIoU"] = validate_seg(model, val_seg_loader, device)

    if args.tasks in ("all", "cls"):


        cls_ds   = ImageClsDataset(f"{args.data_root}/data/imagenette_160/val",   transform=cls_transform)
        cls_loader   = DataLoader(cls_ds,   batch_size=1, shuffle=False, num_workers=2)

        results["Top1"] = evaluate_cls(model, cls_loader, device)


    print(json.dumps(results, indent=2))

# import sys
# sys.argv = [
#     "eval.py",                       # 通常放檔名，寫什麼都行
#     "--weights",   "/content/best_model.pt",
#     "--data_root", "/content/data",
#     "--tasks",     "all"
# ]



if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--weights",   required=True)
    parser.add_argument("--data_root", required=True)
    parser.add_argument("--tasks",     default="all",
                        choices=["all", "det", "seg", "cls"])
    args = parser.parse_args()
    main(args)





